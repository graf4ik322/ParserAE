#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import aiohttp
import json
import re
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

from ...core.config import config
from ...core.logger import get_logger

logger = get_logger("perfume_bot.parser")

@dataclass
class PerfumeData:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä—Ñ—é–º–∞"""
    name: str
    brand: str
    factory: str
    url: str
    price: Optional[float] = None
    volume: Optional[str] = None
    description: Optional[str] = None
    article: Optional[str] = None
    category: Optional[str] = None
    gender: Optional[str] = None
    notes: List[str] = None
    availability: bool = True
    image_url: Optional[str] = None
    rating: Optional[float] = None
    reviews_count: Optional[int] = None
    
    def __post_init__(self):
        if self.notes is None:
            self.notes = []

class PerfumeScraper:
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ø–∞—Ä—Ñ—é–º–æ–≤"""
    
    def __init__(self):
        self.session = None
        self.parsed_urls = set()
        self.perfumes = []
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.max_concurrent = config.parser.max_concurrent_requests
        self.request_delay = config.parser.request_delay
        self.timeout = config.parser.timeout
        self.retry_attempts = config.parser.retry_attempts
        self.user_agent = config.parser.user_agent
        
        # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è concurrent –∑–∞–ø—Ä–æ—Å–æ–≤
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
    
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        connector = aiohttp.TCPConnector(limit=self.max_concurrent)
        
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers={'User-Agent': self.user_agent}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        if self.session:
            await self.session.close()
    
    async def _fetch_with_retry(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(self.retry_attempts):
            try:
                async with self.semaphore:
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            content = await response.text()
                            logger.debug(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
                            return content
                        else:
                            logger.warning(f"‚ö†Ô∏è HTTP {response.status} –¥–ª—è {url}")
                            
            except asyncio.TimeoutError:
                logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç –¥–ª—è {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            
            if attempt < self.retry_attempts - 1:
                await asyncio.sleep(self.request_delay * (attempt + 1))
        
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url} –ø–æ—Å–ª–µ {self.retry_attempts} –ø–æ–ø—ã—Ç–æ–∫")
        return None
    
    async def get_sitemap_urls(self, sitemap_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑ sitemap"""
        logger.info(f"üìã –ü–æ–ª—É—á–µ–Ω–∏–µ URL –∏–∑ sitemap: {sitemap_url}")
        
        content = await self._fetch_with_retry(sitemap_url)
        if not content:
            return []
        
        urls = []
        
        # –ü–∞—Ä—Å–∏–º XML sitemap
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ XML —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
            url_pattern = r'<loc>(.*?)</loc>'
            found_urls = re.findall(url_pattern, content)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º URL –ø–∞—Ä—Ñ—é–º–æ–≤
            perfume_urls = [
                url for url in found_urls 
                if self._is_perfume_url(url)
            ]
            
            urls.extend(perfume_urls)
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(urls)} URL –ø–∞—Ä—Ñ—é–º–æ–≤ –≤ sitemap")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ sitemap: {e}")
        
        return urls
    
    def _is_perfume_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π –ø–∞—Ä—Ñ—é–º–∞"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä—Ñ—é–º–æ–≤
        perfume_patterns = [
            r'/parfum/',
            r'/perfume/',
            r'/fragrance/',
            r'/product/',
            r'\.html$'
        ]
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        exclude_patterns = [
            r'/category/',
            r'/brand/',
            r'/search/',
            r'/cart/',
            r'/checkout/',
            r'/account/',
            r'/blog/',
            r'/news/'
        ]
        
        url_lower = url.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        for pattern in exclude_patterns:
            if re.search(pattern, url_lower):
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –ø–∞—Ä—Ñ—é–º–æ–≤
        for pattern in perfume_patterns:
            if re.search(pattern, url_lower):
                return True
        
        return False
    
    async def parse_perfume_page(self, url: str) -> Optional[PerfumeData]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–∞—Ä—Ñ—é–º–∞"""
        if url in self.parsed_urls:
            return None
        
        content = await self._fetch_with_retry(url)
        if not content:
            return None
        
        self.parsed_urls.add(url)
        
        try:
            soup = BeautifulSoup(content, 'html.parser')
            perfume_data = await self._extract_perfume_data(soup, url)
            
            if perfume_data and perfume_data.name and perfume_data.brand:
                logger.debug(f"‚úÖ –ü–∞—Ä—Ñ—é–º —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω: {perfume_data.brand} - {perfume_data.name}")
                return perfume_data
            else:
                logger.debug(f"‚ö†Ô∏è –ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {url}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")
            return None
    
    async def _extract_perfume_data(self, soup: BeautifulSoup, url: str) -> Optional[PerfumeData]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–∞—Ä—Ñ—é–º–∞ –∏–∑ HTML"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
            name = self._extract_name(soup)
            if not name:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±—Ä–µ–Ω–¥
            brand = self._extract_brand(soup, name)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–±—Ä–∏–∫—É
            factory = self._extract_factory(soup)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            price = self._extract_price(soup)
            volume = self._extract_volume(soup)
            description = self._extract_description(soup)
            article = self._extract_article(soup)
            category = self._extract_category(soup)
            gender = self._extract_gender(soup, name)
            notes = self._extract_notes(soup)
            availability = self._extract_availability(soup)
            image_url = self._extract_image_url(soup, url)
            rating = self._extract_rating(soup)
            reviews_count = self._extract_reviews_count(soup)
            
            return PerfumeData(
                name=name,
                brand=brand,
                factory=factory,
                url=url,
                price=price,
                volume=volume,
                description=description,
                article=article,
                category=category,
                gender=gender,
                notes=notes,
                availability=availability,
                image_url=image_url,
                rating=rating,
                reviews_count=reviews_count
            )
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None
    
    def _extract_name(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä—Ñ—é–º–∞"""
        selectors = [
            'h1.product-title',
            'h1.title',
            'h1',
            '.product-name',
            '.perfume-name',
            '[data-name]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                name = element.get_text(strip=True)
                if name and len(name) > 3:
                    return self._clean_name(name)
        
        return None
    
    def _extract_brand(self, soup: BeautifulSoup, name: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±—Ä–µ–Ω–¥ –ø–∞—Ä—Ñ—é–º–∞"""
        # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∏–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        selectors = [
            '.brand-name',
            '.manufacturer',
            '.product-brand',
            '[data-brand]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                brand = element.get_text(strip=True)
                if brand:
                    return brand
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
        return self._extract_brand_from_name(name)
    
    def _extract_brand_from_name(self, name: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±—Ä–µ–Ω–¥ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        # –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤
        known_brands = [
            'Tom Ford', 'Chanel', 'Dior', 'Gucci', 'Versace', 'Armani',
            'Calvin Klein', 'Hugo Boss', 'Dolce & Gabbana', 'Prada',
            'Yves Saint Laurent', 'Givenchy', 'Burberry', 'Herm√®s',
            'Creed', 'Maison Margiela', 'Byredo', 'Le Labo', 'Amouage'
        ]
        
        name_lower = name.lower()
        for brand in known_brands:
            if brand.lower() in name_lower:
                return brand
        
        # –ï—Å–ª–∏ –±—Ä–µ–Ω–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω, –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
        words = name.split()
        return words[0] if words else "Unknown"
    
    def _extract_factory(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–±—Ä–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è"""
        selectors = [
            '.factory',
            '.manufacturer-factory',
            '.producer'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                factory = element.get_text(strip=True)
                if factory:
                    return factory
        
        return "Unknown Factory"
    
    def _extract_price(self, soup: BeautifulSoup) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É"""
        selectors = [
            '.price',
            '.product-price',
            '.cost',
            '[data-price]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                price_text = element.get_text(strip=True)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                price_match = re.search(r'(\d+(?:\.\d+)?)', price_text.replace(',', '.'))
                if price_match:
                    try:
                        return float(price_match.group(1))
                    except ValueError:
                        continue
        
        return None
    
    def _extract_volume(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—ä–µ–º"""
        selectors = [
            '.volume',
            '.size',
            '.ml'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                volume = element.get_text(strip=True)
                if 'ml' in volume.lower() or '–º–ª' in volume.lower():
                    return volume
        
        # –ü–æ–∏—Å–∫ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–∏
        text = soup.get_text()
        volume_match = re.search(r'(\d+\s*(?:ml|–º–ª))', text, re.IGNORECASE)
        if volume_match:
            return volume_match.group(1)
        
        return None
    
    def _extract_description(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ"""
        selectors = [
            '.description',
            '.product-description',
            '.about',
            '.details'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                description = element.get_text(strip=True)
                if description and len(description) > 20:
                    return description[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        
        return None
    
    def _extract_article(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—Ä—Ç–∏–∫—É–ª"""
        selectors = [
            '.article',
            '.sku',
            '.product-code',
            '[data-sku]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                article = element.get_text(strip=True)
                if article:
                    return article
        
        return None
    
    def _extract_category(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é"""
        selectors = [
            '.category',
            '.breadcrumb',
            '.product-category'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                category = element.get_text(strip=True)
                if category:
                    return category
        
        return None
    
    def _extract_gender(self, soup: BeautifulSoup, name: str) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–ª –ø–∞—Ä—Ñ—é–º–∞"""
        text = (soup.get_text() + " " + name).lower()
        
        if any(word in text for word in ['–º—É–∂—Å–∫–æ–π', 'for men', 'homme', 'male']):
            return 'male'
        elif any(word in text for word in ['–∂–µ–Ω—Å–∫–∏–π', 'for women', 'femme', 'female']):
            return 'female'
        elif any(word in text for word in ['—É–Ω–∏—Å–µ–∫—Å', 'unisex', 'for all']):
            return 'unisex'
        
        return None
    
    def _extract_notes(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ—Ç—ã –ø–∞—Ä—Ñ—é–º–∞"""
        notes = []
        
        selectors = [
            '.notes',
            '.fragrance-notes',
            '.composition'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                notes_text = element.get_text()
                # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ—Ç —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—ã–µ
                note_list = [note.strip() for note in notes_text.split(',')]
                notes.extend(note_list)
        
        return notes[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ—Ç
    
    def _extract_availability(self, soup: BeautifulSoup) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ç–æ–≤–∞—Ä–∞"""
        unavailable_indicators = [
            'out of stock',
            '–Ω–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏',
            '–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
            'sold out'
        ]
        
        text = soup.get_text().lower()
        return not any(indicator in text for indicator in unavailable_indicators)
    
    def _extract_image_url(self, soup: BeautifulSoup, base_url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        selectors = [
            '.product-image img',
            '.perfume-image img',
            '.main-image img',
            'img.product'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                src = element.get('src') or element.get('data-src')
                if src:
                    return urljoin(base_url, src)
        
        return None
    
    def _extract_rating(self, soup: BeautifulSoup) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥"""
        selectors = [
            '.rating',
            '.stars',
            '[data-rating]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                rating_text = element.get_text(strip=True)
                rating_match = re.search(r'(\d+(?:\.\d+)?)', rating_text)
                if rating_match:
                    try:
                        rating = float(rating_match.group(1))
                        if 0 <= rating <= 5:
                            return rating
                    except ValueError:
                        continue
        
        return None
    
    def _extract_reviews_count(self, soup: BeautifulSoup) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤"""
        selectors = [
            '.reviews-count',
            '.review-count',
            '[data-reviews]'
        ]
        
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                count_text = element.get_text(strip=True)
                count_match = re.search(r'(\d+)', count_text)
                if count_match:
                    try:
                        return int(count_match.group(1))
                    except ValueError:
                        continue
        
        return None
    
    def _clean_name(self, name: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã
        name = re.sub(r'\s+', ' ', name.strip())
        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å
        name = re.sub(r'<[^>]+>', '', name)
        return name
    
    async def scrape_all_perfumes(self, sitemap_url: str = None) -> List[PerfumeData]:
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ –ø–∞—Ä—Ñ—é–º—ã —Å —Å–∞–π—Ç–∞"""
        if not sitemap_url:
            sitemap_url = config.data.perfume_catalog_url
        
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –ø–∞—Ä—Ñ—é–º–æ–≤ —Å {sitemap_url}")
        
        # –ü–æ–ª—É—á–∞–µ–º URL –∏–∑ sitemap
        urls = await self.get_sitemap_urls(sitemap_url)
        if not urls:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å URL –∏–∑ sitemap")
            return []
        
        logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(urls)} URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
        
        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = []
        for url in urls[:100]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            task = asyncio.create_task(self.parse_perfume_page(url))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        perfumes = []
        for result in results:
            if isinstance(result, PerfumeData):
                perfumes.append(result)
            elif isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {result}")
        
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {len(perfumes)} –ø–∞—Ä—Ñ—é–º–æ–≤")
        self.perfumes = perfumes
        return perfumes
    
    def save_to_json(self, filename: str = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–∞–π–ª"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"perfumes_raw_{timestamp}.json"
        
        filepath = config.get_data_file_path(filename, processed=False)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è JSON
        data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_count": len(self.perfumes),
                "parser_version": "2.0.0"
            },
            "perfumes": [asdict(perfume) for perfume in self.perfumes]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")
        return str(filepath)